model_params = {
    "n_latent": 64,
    "recon_loss": "nb",
    "doser_type": "linear",
    "n_hidden_encoder": 128,
    "n_layers_encoder": 2,
    "n_hidden_decoder": 512,
    "n_layers_decoder": 2,
    "use_batch_norm_encoder": True,
    "use_layer_norm_encoder": False,
    "use_batch_norm_decoder": False,
    "use_layer_norm_decoder": True,
    "dropout_rate_encoder": 0.0,
    "dropout_rate_decoder": 0.1,
    "variational": False,
    "seed": 6977,
    "use_intense": True,
    "use_rite": True,
    "intense_reg_rate": 0.05,
    "intense_p": 1,
    "interaction_order": 2,
    "rite_factor": 0.9,
}

trainer_params = {
    "n_epochs_adv_warmup": 10,
    "n_epochs_kl_warmup": None,
    "n_epochs_pretrain_ae": 30,
    "adv_steps": 2,
    "mixup_alpha": 0.2,
    "n_epochs_mixup_warmup": 10,
    "n_layers_adv": 2,
    "n_hidden_adv": 64,
    "use_batch_norm_adv": True,
    "use_layer_norm_adv": False,
    "dropout_rate_adv": 0.25,
    "pen_adv": 0.21176187723505563,
    "reg_adv": 3.3191059919680557,
    "lr": 0.00656775854452724,
    "wd": 0.00000017930296245015,
    "doser_lr": 0.00034816913906047706,
    "doser_wd": 0.00000139054966251716,
    "adv_lr": 0.0005237774528387638,
    "adv_wd": 0.00000015681818353966,
    "adv_loss": "cce",
    "do_clip_grad": True,
    "gradient_clip_value": 1.0,
    "step_size_lr": 25,
    "momentum": 0.9007079992500057
}