model_params = {
    "n_latent": 64,
    "recon_loss": "nb",
    "doser_type": "linear",
    "n_hidden_encoder": 128,
    "n_layers_encoder": 2,
    "n_hidden_decoder": 512,
    "n_layers_decoder": 2,
    "use_batch_norm_encoder": True,
    "use_layer_norm_encoder": False,
    "use_batch_norm_decoder": False,
    "use_layer_norm_decoder": True,
    "dropout_rate_encoder": 0.0,
    "dropout_rate_decoder": 0.1,
    "variational": False,
    "seed": 6977,
    "use_intense": True,
    "use_rite": True,
    "intense_reg_rate": 0.1,
    "intense_p": 1,
    "interaction_order": 2,
    "rite_factor": 1,
}

trainer_params = {
    "n_epochs_adv_warmup": 5,
    "n_epochs_kl_warmup": None,
    "n_epochs_pretrain_ae": 10,
    "adv_steps": 5,
    "mixup_alpha": 0.1,
    "n_epochs_mixup_warmup": 3,
    "n_layers_adv": 4,
    "n_hidden_adv": 64,
    "use_batch_norm_adv": True,
    "use_layer_norm_adv": False,
    "dropout_rate_adv": 0.3,
    "pen_adv": 1.0754212899512865,
    "reg_adv": 1.7536582421567608,
    "lr": 0.00610331507808799,
    "wd": 0.00000001500803976284,
    "doser_lr": 0.004392578706283446,
    "doser_wd": 0.00000263511691407522,
    "adv_lr": 0.0000532609227647633,
    "adv_wd": 0.00000010496500499614,
    "adv_loss": "cce",
    "do_clip_grad": True,
    "gradient_clip_value": 1.0,
    "step_size_lr": 25,
    "momentum": 0.8381474745171963
}