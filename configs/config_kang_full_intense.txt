model_params = {
        "n_latent": 64,
        "recon_loss": "nb",
        "doser_type": "linear",
        "n_hidden_encoder": 128,
        "n_layers_encoder": 2,
        "n_hidden_decoder": 512,
        "n_layers_decoder": 2,
        "use_batch_norm_encoder": True,
        "use_layer_norm_encoder": False,
        "use_batch_norm_decoder": False,
        "use_layer_norm_decoder": True,
        "dropout_rate_encoder": 0.0,
        "dropout_rate_decoder": 0.1,
        "variational": False,
        "seed": args.seed,
        "use_intense": True,
        "intense_reg_rate": args.intense_reg_rate,
        "intense_p": args.intense_p
    }
trainer_params = {
        "n_epochs_adv_warmup": 5,
        "n_epochs_kl_warmup": None,
        "n_epochs_pretrain_ae": 1,
        "adv_steps": 2,
        "mixup_alpha": 0.2,
        "n_epochs_mixup_warmup": 1,
        "n_layers_adv": 3,
        "n_hidden_adv": 32,
        "use_batch_norm_adv": False,
        "use_layer_norm_adv": False,
        "dropout_rate_adv": 0.25,
        "pen_adv": 39.346720337841475,
        "reg_adv": 5.8828093187758315,
        "lr": 0.00004749152349939709,
        "wd": 5.058319112901893e-07,
        "doser_lr": 0.007730892767217309,
        "doser_wd": 0.00000998035103488644,
        "adv_lr": 0.0006626219830664296,
        "adv_wd": 0.00000389001091098564,
        "adv_loss": "cce",
        "do_clip_grad": False,
        "gradient_clip_value": 1.0,
        "step_size_lr": 25,
    }