model_params = {
        "n_latent": 32,
        "recon_loss": "nb",
        "doser_type": "linear",
        "n_hidden_encoder": 256,
        "n_layers_encoder": 4,
        "n_hidden_decoder": 256,
        "n_layers_decoder": 2,
        "use_batch_norm_encoder": True,
        "use_layer_norm_encoder": False,
        "use_batch_norm_decoder": False,
        "use_layer_norm_decoder": False,
        "dropout_rate_encoder": 0.2,
        "dropout_rate_decoder": 0.0,
        "variational": False,
        "seed": args.seed,
        "use_intense": True,
        "intense_reg_rate": args.intense_reg_rate,
        "intense_p": args.intense_p
    }
    trainer_params = {
        "n_epochs_kl_warmup": None,
        "n_epochs_adv_warmup": 1,
        "n_epochs_mixup_warmup": 0,
        "n_epochs_pretrain_ae": 10,
        "mixup_alpha": 0.2,
        "lr": 0.00002252440063929891,
        "wd": 0.00000896884743064553,
        "adv_steps": 2,
        "reg_adv": 57.60772464491533,
        "pen_adv": 0.07291543074894648,
        "adv_lr": 0.008080384893740146,
        "adv_wd": 0.00000051119688291297,
        "doser_lr": 0.00001557068423713765,
        "doser_wd": 0.00000338530031517114,
        "n_layers_adv": 2,
        "n_hidden_adv": 64,
        "use_batch_norm_adv": True,
        "use_layer_norm_adv": False,
        "dropout_rate_adv": 0.25,
        "step_size_lr": 45,
        "do_clip_grad": False,
        "adv_loss": "cce",
        "gradient_clip_value": 1.0,
        "momentum": 0.8553542769334178
    }