{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf8a9058-67e6-40f2-b855-7a5a6776b7a7",
   "metadata": {},
   "source": [
    "# Predicting single-cell response to unseen  combinatorial CRISPR perturbations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d2c95de",
   "metadata": {},
   "source": [
    "In this tutorial, we will train and evaluate a CPA model on the [Norman 2019](https://www.google.com/search?q=norman+genetic+manifold&oq=norman+genetic+manifold&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDMyMThqMGo3qAIAsAIA&sourceid=chrome&ie=UTF-8) dataset. See the last [Figure 5](https://www.embopress.org/doi/full/10.15252/msb.202211517) in the CPA paper. \n",
    "\n",
    "The goal is to predict gene expression response to perturbation responses of `X+Y` when you have seen single cells from `X` and `Y`. You can extend this model to predict \n",
    "`X+Y` when either `X`, `Y`, or both are unseen. In this scenario, you need to use external embedding for your favourite gene representations (see an example [here](https://cpa-tools.readthedocs.io/en/latest/tutorials/combosciplex_Rdkit_embeddings.html#))\n",
    "\n",
    "The following steps are going to be covered:\n",
    "1. Setting up environment\n",
    "2. Loading the dataset\n",
    "3. Preprocessing the dataset\n",
    "4. Creating a CPA model\n",
    "5. Training the model\n",
    "6. Latent space visualisation\n",
    "7. Prediction evaluation across different perturbations"
   ]
  },
  {
   "cell_type": "code",
   "id": "fb784181-b262-4822-bd8b-c718072b678f",
   "metadata": {},
   "source": [
    "import sys\n",
    "#if branch is stable, will install via pypi, else will install from source\n",
    "branch = \"latest\"\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB and branch == \"stable\":\n",
    "    !pip install cpa-tools\n",
    "    !pip install scanpy\n",
    "elif IN_COLAB and branch != \"stable\":\n",
    "    !pip install --quiet --upgrade jsonschema\n",
    "    !pip install git+https://github.com/theislab/cpa\n",
    "    !pip install scanpy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1c687c31-96f4-49cc-8429-ef774c966c8a",
   "metadata": {},
   "source": [
    "import os\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "os.chdir(\"../..\")\n",
    "# Verify the new directory\n",
    "print(\"New directory:\", os.getcwd())\n",
    "current_dir = os.getcwd()\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38be1499-86dc-4465-873f-b994e5f4a21a",
   "metadata": {},
   "source": [
    "import cpa\n",
    "import scanpy as sc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f18cb22-1c6e-4329-b965-1fcabd978dbe",
   "metadata": {},
   "source": [
    "sc.settings.set_figure_params(dpi=100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "504bd1f9-945e-4ee1-ac7b-4f6ea19fb2a6",
   "metadata": {},
   "source": [
    "data_path = os.path.join(current_dir, \"datasets\", \"Norman2019_normalized_hvg.h5ad\")\n",
    "#data_path = '/data/mohsen/scPert/scPerturb/Norman2019_normalized_hvg.h5ad'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "491261f0",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "\n",
    "The preprocessed Norman et. al 2019 dataset with `h5ad` extension used for saving/loading anndata objects is publicly available in the [Google Drive](https://drive.google.com/drive/folders/1pxT0fvXtqBBtdv1CCPVwJaMLHe9XpMHo?usp=sharing) and can be loaded using the `sc.read` function with the `backup_url` argument."
   ]
  },
  {
   "cell_type": "code",
   "id": "d4e06d9e-ecb3-4014-a03d-80ea8b4d8820",
   "metadata": {},
   "source": [
    "try:\n",
    "    adata = sc.read(data_path)\n",
    "except:\n",
    "    import gdown\n",
    "    gdown.download('https://drive.google.com/uc?export=download&id=109G9MmL-8-uh7OSjnENeZ5vFbo62kI7j')\n",
    "    data_path = 'Norman2019_normalized_hvg.h5ad'\n",
    "    adata = sc.read(data_path)\n",
    "\n",
    "adata"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f53d710f",
   "metadata": {},
   "source": [
    "Next, we just replace `adata.X` with raw counts to be able to train CPA with Negative Binomial (aka NB) loss."
   ]
  },
  {
   "cell_type": "code",
   "id": "8a3e5357",
   "metadata": {},
   "source": [
    "adata.X = adata.layers['counts'].copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67374fdb",
   "metadata": {},
   "source": [
    "## Pre-processing Dataset\n",
    "Preprocessing is the first step required for training a model. Just like scvi-tools models, you can call `cpa.CPA.setup_anndata` to preprocess your data. This function will accept the following arguments:\n",
    "- `adata`: AnnData object containing the data to be preprocessed\n",
    "- `perturbation_key`: The key in `adata.obs` that contains the perturbation information\n",
    "- `control_group`: The name of the control group in `perturbation_key`\n",
    "- `batch_key`: The key in `adata.obs` that contains the batch information\n",
    "- `dosage_key`: The key in `adata.obs` that contains the dosage information\n",
    "- `categorical_covariate_keys`: A list of keys in `adata.obs` that contain categorical covariates\n",
    "- `is_count_data`: Whether the `adata.X` is count data or not\n",
    "- `deg_uns_key`: The key in `adata.uns` that contains the differential expression results\n",
    "- `deg_uns_cat_key`: The key in `adata.obs` that contains the category information of each cell which can be used as to access differential expression results in `adata.uns[deg_uns_key]`. For example, if `deg_uns_key` is `rank_genes_groups_cov` and `deg_uns_cat_key` is `cov_cond`, then `adata.uns[deg_uns_key][cov_cond]` will contain the differential expression results for each category in `cov_cond`.\n",
    "- `max_comb_len`: The maximum number of perturbations that are applied to each cell. For example, if `max_comb_len` is 2, then the model will be trained to predict the effect of single perturbations and the effect of double perturbations."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb8f237c-1efb-45e6-a089-266e333a603d",
   "metadata": {},
   "source": [
    "cpa.CPA.setup_anndata(adata, \n",
    "                      perturbation_key='cond_harm',\n",
    "                      control_group='ctrl',\n",
    "                      dosage_key='dose_value',\n",
    "                      categorical_covariate_keys=['cell_type'],\n",
    "                      is_count_data=True,\n",
    "                      deg_uns_key='rank_genes_groups_cov',\n",
    "                      deg_uns_cat_key='cov_cond',\n",
    "                      max_comb_len=2,\n",
    "                     )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6fcc8a93-e430-4ad4-9681-60589edd4d9c",
   "metadata": {},
   "source": [
    "model_params = {\n",
    "    \"n_latent\": 32,\n",
    "    \"recon_loss\": \"nb\",\n",
    "    \"doser_type\": \"linear\",\n",
    "    \"n_hidden_encoder\": 256,\n",
    "    \"n_layers_encoder\": 4,\n",
    "    \"n_hidden_decoder\": 256,\n",
    "    \"n_layers_decoder\": 2,\n",
    "    \"use_batch_norm_encoder\": True,\n",
    "    \"use_layer_norm_encoder\": False,\n",
    "    \"use_batch_norm_decoder\": False,\n",
    "    \"use_layer_norm_decoder\": False,\n",
    "    \"dropout_rate_encoder\": 0.2,\n",
    "    \"dropout_rate_decoder\": 0.0,\n",
    "    \"variational\": False,\n",
    "    \"seed\": 8206,\n",
    "}\n",
    "\n",
    "trainer_params = {\n",
    "    \"n_epochs_kl_warmup\": None,\n",
    "    \"n_epochs_adv_warmup\": 50,\n",
    "    \"n_epochs_mixup_warmup\": 10,\n",
    "    \"n_epochs_pretrain_ae\": 10,\n",
    "    \"mixup_alpha\": 0.1,\n",
    "    \"lr\": 0.0001,\n",
    "    \"wd\": 3.2170178270865573e-06,\n",
    "    \"adv_steps\": 3,\n",
    "    \"reg_adv\": 10.0,\n",
    "    \"pen_adv\": 20.0,\n",
    "    \"adv_lr\": 0.0001,\n",
    "    \"adv_wd\": 7.051355554517135e-06,\n",
    "    \"n_layers_adv\": 2,\n",
    "    \"n_hidden_adv\": 128,\n",
    "    \"use_batch_norm_adv\": True,\n",
    "    \"use_layer_norm_adv\": False,\n",
    "    \"dropout_rate_adv\": 0.3,\n",
    "    \"step_size_lr\": 25,\n",
    "    \"do_clip_grad\": False,\n",
    "    \"adv_loss\": \"cce\",\n",
    "    \"gradient_clip_value\": 5.0,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "09726a17",
   "metadata": {},
   "source": [
    "Dataset split: We leave DUSP9+ETS2 and CNN1+CBL out of training dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "242ead8f",
   "metadata": {},
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0bcbef24",
   "metadata": {},
   "source": [
    "adata.obs['split'] = np.random.choice(['train', 'valid'], size=adata.n_obs, p=[0.85, 0.15])\n",
    "adata.obs.loc[adata.obs['cond_harm'].isin(['DUSP9+ETS2', 'CBL+CNN1']), 'split'] = 'ood'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "47ab396a",
   "metadata": {},
   "source": [
    "adata.obs['split'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67384d6c",
   "metadata": {},
   "source": [
    "## CPA Model\n",
    "\n",
    "You can create a CPA model by creating an object from `cpa.CPA` class. The constructor of this class takes the following arguments:\n",
    "**Data related parameters:** \n",
    "- `adata`: AnnData object containing train/valid/test data\n",
    "- `split_key`: The key in `adata.obs` that contains the split information\n",
    "- `train_split`: The value in `split_key` that corresponds to the training data\n",
    "- `valid_split`: The value in `split_key` that corresponds to the validation data\n",
    "- `test_split`: The value in `split_key` that corresponds to the test data\n",
    "**Model architecture parameters:**\n",
    "- `n_latent`: Number of latent dimensions\n",
    "- `recon_loss`: Reconstruction loss function. Currently, Supported losses are `nb`, `zinb`, and `gauss`.\n",
    "- `n_hidden_encoder`: Number of hidden units in the encoder\n",
    "- `n_layers_encoder`: Number of layers in the encoder\n",
    "- `n_hidden_decoder`: Number of hidden units in the decoder\n",
    "- `n_layers_decoder`: Number of layers in the decoder\n",
    "- `use_batch_norm_encoder`: Whether to use batch normalization in the encoder\n",
    "- `use_layer_norm_encoder`: Whether to use layer normalization in the encoder\n",
    "- `use_batch_norm_decoder`: Whether to use batch normalization in the decoder\n",
    "- `use_layer_norm_decoder`: Whether to use layer normalization in the decoder\n",
    "- `dropout_rate_encoder`: Dropout rate in the encoder\n",
    "- `dropout_rate_decoder`: Dropout rate in the decoder\n",
    "- `variational`: Whether to use variational inference. NOTE: False is highly recommended.\n",
    "- `seed`: Random seed"
   ]
  },
  {
   "cell_type": "code",
   "id": "e50d3962",
   "metadata": {},
   "source": [
    "model = cpa.CPA(adata=adata, \n",
    "                split_key='split',\n",
    "                train_split='train',\n",
    "                valid_split='valid',\n",
    "                test_split='ood',\n",
    "                **model_params,\n",
    "               )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1103fdf",
   "metadata": {},
   "source": [
    "## Training CPA\n",
    "\n",
    "In order to train your CPA model, you need to use `train` function of your `model`. This function accepts the following parameters:\n",
    "- `max_epochs`: Maximum number of epochs to train the model. CPA generally converges after high number of epochs, so you can set this to a high value.\n",
    "- `use_gpu`: If you have a GPU, you can set this to `True` to speed up the training process.\n",
    "- `batch_size`: Batch size for training. You can set this to a high value (e.g. 512, 1024, 2048) if you have a GPU. \n",
    "- `plan_kwargs`: dictionary of parameters passed the CPA's `TrainingPlan`. You can set the following parameters:\n",
    "    * `n_epochs_adv_warmup`: Number of epochs to linearly increase the weight of adversarial loss. \n",
    "    * `n_epochs_mixup_warmup`: Number of epochs to linearly increase the weight of mixup loss.\n",
    "    * `n_epochs_pretrain_ae`: Number of epochs to pretrain the autoencoder.\n",
    "    * `lr`: Learning rate for training autoencoder.\n",
    "    * `wd`: Weight decay for training autoencoder.\n",
    "    * `adv_lr`: Learning rate for training adversary.\n",
    "    * `adv_wd`: Weight decay for training adversary.\n",
    "    * `adv_steps`: Number of steps to train adversary for each step of autoencoder.\n",
    "    * `reg_adv`: Maximum Weight of adversarial loss.\n",
    "    * `pen_adv`: Penalty weight of adversarial loss.\n",
    "    * `n_layers_adv`: Number of layers in adversary.\n",
    "    * `n_hidden_adv`: Number of hidden units in adversary.\n",
    "    * `use_batch_norm_adv`: Whether to use batch normalization in adversary.\n",
    "    * `use_layer_norm_adv`: Whether to use layer normalization in adversary.\n",
    "    * `dropout_rate_adv`: Dropout rate in adversary.\n",
    "    * `step_size_lr`: Step size for learning rate scheduler.\n",
    "    * `do_clip_grad`: Whether to clip gradients by norm.\n",
    "    * `clip_grad_value`: Maximum value of gradient norm.\n",
    "    * `adv_loss`: Type of adversarial loss. Can be either `cce` for Cross Entropy loss or `focal` for Focal loss.\n",
    "    * `n_epochs_verbose`: Number of epochs to print latent information disentanglement evaluation.\n",
    "- `early_stopping_patience`: Number of epochs to wait before stopping training if validation metric does not improve.\n",
    "- `check_val_every_n_epoch`: Number of epochs to wait before running validation.\n",
    "- `save_path`: Path to save the best model after training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0f6ca89-9886-449f-a9c3-5c2421883a41",
   "metadata": {
    "tags": []
   },
   "source": [
    "model.train(max_epochs=2000,\n",
    "            use_gpu=True, \n",
    "            batch_size=2048,\n",
    "            plan_kwargs=trainer_params,\n",
    "            early_stopping_patience=5,\n",
    "            check_val_every_n_epoch=5,\n",
    "            save_path=os.path.join(current_dir,'lightning_logs','Norman2019' )\n",
    "           )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d71e0099-912b-402e-afb6-a3f920076b3d",
   "metadata": {},
   "source": [
    "cpa.pl.plot_history(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2deb470",
   "metadata": {},
   "source": [
    "## Restore best model\n",
    "\n",
    "In case you have already saved your pretrained model, you can restore it using the following code. The `cpa.CPA.load` function accepts the following arguments:\n",
    "- `dir_path`: path to the directory where the model is saved\n",
    "- `adata`: anndata object\n",
    "- `use_gpu`: whether to use GPU or not\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ddb437f6",
   "metadata": {},
   "source": [
    "\n",
    "# model = cpa.CPA.load(dir_path='/home/mohsen/projects/cpa/lightning_logs/Norman2019/',\n",
    "#                      adata=adata,\n",
    "#                      use_gpu=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e565339b",
   "metadata": {},
   "source": [
    "## Latent Space Visualization\n",
    "\n",
    "latent vectors of all cells can be computed with `get_latent_representation` function. This function produces a python dictionary with the following keys:\n",
    "- `latent_basal`: latent vectors of all cells in basal state of autoencoder\n",
    "- `latent_after`: final latent vectors which can be used for decoding\n",
    "- `latent_corrected`: batch-corrected latents if batch_key was provided"
   ]
  },
  {
   "cell_type": "code",
   "id": "769e8c82",
   "metadata": {},
   "source": [
    "latent_outputs = model.get_latent_representation(adata, batch_size=2048)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f158465",
   "metadata": {},
   "source": [
    "latent_outputs.keys()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3fe6c925",
   "metadata": {},
   "source": [
    "sc.pp.neighbors(latent_outputs['latent_basal'])\n",
    "sc.tl.umap(latent_outputs['latent_basal'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bef42749",
   "metadata": {},
   "source": [
    "groups = list(np.unique(adata[adata.obs['split'] == 'ood'].obs['cond_harm'].values))\n",
    "len(groups)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61126a74-54d0-4e39-a576-49a856847130",
   "metadata": {},
   "source": [
    "As observed below, the basal representation should be free of the variation(s) of the `cond_harm`"
   ]
  },
  {
   "cell_type": "code",
   "id": "0bf1a262",
   "metadata": {},
   "source": [
    "sc.pl.umap(latent_outputs['latent_basal'], \n",
    "           color='cond_harm', \n",
    "           groups=groups,\n",
    "           palette=sc.pl.palettes.godsnot_102,\n",
    "           frameon=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17718660-7ac0-4201-99ef-b200f6f0ae45",
   "metadata": {},
   "source": [
    "We can further color them by the gene programs that each perturbation will induce"
   ]
  },
  {
   "cell_type": "code",
   "id": "750411f5",
   "metadata": {},
   "source": [
    "sc.pl.umap(latent_outputs['latent_basal'], \n",
    "           color='pathway', \n",
    "           palette=sc.pl.palettes.godsnot_102,\n",
    "           frameon=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98b402fb",
   "metadata": {},
   "source": [
    "sc.pp.neighbors(latent_outputs['latent_after'])\n",
    "sc.tl.umap(latent_outputs['latent_after'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cb36efcb-0dda-440d-821e-ecafa5c0969c",
   "metadata": {},
   "source": [
    "Here, you can visualise that when gene embeddings are added to the basal representation, the cells treated with different drugs will be separated."
   ]
  },
  {
   "cell_type": "code",
   "id": "8522d21f",
   "metadata": {},
   "source": [
    "sc.pl.umap(latent_outputs['latent_after'], \n",
    "           color='cond_harm', \n",
    "           groups=groups,\n",
    "           palette=sc.pl.palettes.godsnot_102,\n",
    "           frameon=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99857bcf",
   "metadata": {},
   "source": [
    "sc.pl.umap(latent_outputs['latent_after'], \n",
    "           color='pathway', \n",
    "           palette=sc.pl.palettes.godsnot_102,\n",
    "           frameon=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "911b8cf7-ef8e-4a9b-bc2a-9d78b204508c",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f97a0e",
   "metadata": {},
   "source": [
    "To evaluate the model's prediction performance, we can use `model.predict()` function. $R^2$ score for each genetic interaction (GI) is computed over mean statistics of the top 50, 20, and 10 DEGs (including all genes). CPA transfers the context from control to GI-perturbed for K562 cells. Next, we will evaluate the model's prediction performance on the whole dataset, including OOD (test) cells. The model will report metrics on how well we have\n",
    "captured the variation in top `n` differentially expressed genes when compared to control cells\n",
    "(`CTRL`)  for each condition. The metrics calculate the mean accuracy (`r2_mean_deg`) and mean log-fold-change accuracy (`r2_mean_lfc_deg`).  The `R2` is the `sklearn.metrics.r2_score` from [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b4b41",
   "metadata": {},
   "source": [
    "NOTE: To perform counter-factual prediction, we first need to set `adata.X` to sampled control cells. Then, we can use `model.predict()` function to predict the effect of perturbations on these cells. "
   ]
  },
  {
   "cell_type": "code",
   "id": "61c215a4",
   "metadata": {},
   "source": [
    "adata.layers['X_true'] = adata.X.copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc5c35db",
   "metadata": {},
   "source": [
    "ctrl_adata = adata[adata.obs['cond_harm'] == 'ctrl'].copy()\n",
    "\n",
    "adata.X = ctrl_adata.X[np.random.choice(ctrl_adata.n_obs, size=adata.n_obs, replace=True), :]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ad2bb77",
   "metadata": {},
   "source": [
    "model.predict(adata, batch_size=2048)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10e52a27",
   "metadata": {},
   "source": [
    "adata.layers['CPA_pred'] = adata.obsm['CPA_pred'].copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "151072f9",
   "metadata": {},
   "source": [
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "sc.pp.normalize_total(adata, target_sum=1e4, layer='CPA_pred')\n",
    "sc.pp.log1p(adata, layer='CPA_pred')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d7b88a9",
   "metadata": {},
   "source": [
    "adata.X.max(), adata.layers['CPA_pred'].max()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b43232e",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_top_degs = [10, 20, 50, None] # None means all genes\n",
    "\n",
    "results = defaultdict(list)\n",
    "ctrl_adata = adata[adata.obs['cond_harm'] == 'ctrl'].copy()\n",
    "for condition in tqdm(adata.obs['cond_harm'].unique()):\n",
    "    if condition != 'ctrl':\n",
    "        cond_adata = adata[adata.obs['cond_harm'] == condition].copy()\n",
    "\n",
    "        deg_cat = f'K562_{condition}'\n",
    "        deg_list = adata.uns['rank_genes_groups_cov'][deg_cat]\n",
    "        \n",
    "        x_true = cond_adata.layers['counts'].toarray()\n",
    "        x_pred = cond_adata.obsm['CPA_pred']\n",
    "        x_ctrl = ctrl_adata.layers['counts'].toarray()\n",
    "\n",
    "        for n_top_deg in n_top_degs:\n",
    "            if n_top_deg is not None:\n",
    "                degs = np.where(np.isin(adata.var_names, deg_list[:n_top_deg]))[0]\n",
    "            else:\n",
    "                degs = np.arange(adata.n_vars)\n",
    "                n_top_deg = 'all'\n",
    "                \n",
    "            x_true_deg = x_true[:, degs]\n",
    "            x_pred_deg = x_pred[:, degs]\n",
    "            x_ctrl_deg = x_ctrl[:, degs]\n",
    "            \n",
    "            r2_mean_deg = r2_score(x_true_deg.mean(0), x_pred_deg.mean(0))\n",
    "\n",
    "            r2_mean_lfc_deg = r2_score(x_true_deg.mean(0) - x_ctrl_deg.mean(0), x_pred_deg.mean(0) - x_ctrl_deg.mean(0))\n",
    "            \n",
    "            results['condition'].append(condition)\n",
    "            results['n_top_deg'].append(n_top_deg)\n",
    "            results['r2_mean_deg'].append(r2_mean_deg)\n",
    "            results['r2_mean_lfc_deg'].append(r2_mean_lfc_deg)\n",
    "\n",
    "df = pd.DataFrame(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6561a3cd",
   "metadata": {},
   "source": [
    "df[df['condition'].isin(['DUSP9+ETS2', 'CBL+CNN1'])]"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cpa)",
   "language": "python",
   "name": "cpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
